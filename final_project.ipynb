{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Installation of files and libraries\n",
        "- Install spacy and nltk"
      ],
      "metadata": {
        "id": "IxqTe-con3pV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Installation of Libraries and Downloadind of nlp Machine-Learning Model\n",
        "\n",
        "!pip install pandas\n",
        "!pip install -U spacy\n",
        "\n",
        "!python -m spacy download zh_core_web_md\n",
        "\n",
        "# Move the import statement up here so that it doesn't reinstalled everything everytime a code block runs\n",
        "import nltk\n",
        "nltk.download('popular')\n",
        "nltk.download('tagsets')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5_MddaToAcm",
        "outputId": "67b6e1e6-6fdb-4028-f21c-422e8457345c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.2.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 46.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.1 MB/s \n",
            "\u001b[?25hCollecting thinc<8.1.0,>=8.0.12\n",
            "  Downloading thinc-8.0.13-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (628 kB)\n",
            "\u001b[K     |████████████████████████████████| 628 kB 31.6 MB/s \n",
            "\u001b[?25hCollecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 28.7 MB/s \n",
            "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
            "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.10.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.63.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.5)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Collecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n",
            "\u001b[K     |████████████████████████████████| 451 kB 43.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.6 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 spacy-3.2.3 spacy-legacy-3.0.9 spacy-loggers-1.0.1 srsly-2.4.2 thinc-8.0.13 typer-0.4.0\n",
            "Collecting zh-core-web-md==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/zh_core_web_md-3.2.0/zh_core_web_md-3.2.0-py3-none-any.whl (79.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 79.0 MB 106 kB/s \n",
            "\u001b[?25hCollecting spacy-pkuseg<0.1.0,>=0.0.27\n",
            "  Downloading spacy_pkuseg-0.0.28-cp37-cp37m-manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from zh-core-web-md==3.2.0) (3.2.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (1.21.5)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (1.0.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (57.4.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (0.9.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (8.0.13)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (2.4.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (4.63.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (21.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (0.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (3.0.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: cython>=0.25 in /usr/local/lib/python3.7/dist-packages (from spacy-pkuseg<0.1.0,>=0.0.27->zh-core-web-md==3.2.0) (0.29.28)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (2.0.1)\n",
            "Installing collected packages: spacy-pkuseg, zh-core-web-md\n",
            "Successfully installed spacy-pkuseg-0.0.28 zh-core-web-md-3.2.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('zh_core_web_md')\n",
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw-1.4.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2021.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet31.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Step\n",
        "\n",
        "- Read the column titled “sentence” in dataset1.txt as well as in dataset2.txt into a single list_of_sentences.\n",
        "- Filter out \"v-noise\" from the sentence string elements of the list\n",
        "- Split list_of_sentences into two different array, one contains only Chinese-major sentences, another contains only English-major sentences.\n"
      ],
      "metadata": {
        "id": "SGSHkaZPDjvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing Step\n",
        "\n",
        "# - Read the column titled “sentence” in dataset1.csv as well as in dataset2.csv into a single list_of_sentences.\n",
        "# - Filter out \"v-noise\" from the sentence string elements of the list\n",
        "# - Split list_of_sentences into two different array, one contains only Chinese-major sentences, another contains only English-major sentences.\n",
        "# - Output both array as two seperate datasets titled dataset_eng_major.txt and dataset_ch_major.txt\n",
        "\n",
        "from pandas import *\n",
        "import re \n",
        "\n",
        "#Step 2: read the two csv files into two dataframes by means of the read_csv function\n",
        "# Use a direct filepath so that when datasets are imported, it won't error\n",
        "dataframe_1 = read_csv('dataset1.csv') # DONT CHANGE THESE!\n",
        "dataframe_2 = read_csv('dataset2.csv')\n",
        " \n",
        "#We are reading csv files, not txt files, so we do not employ the open function above, but we can specify the file path in the read_csv function \n",
        "#just as we would in the open function, according to the code snipped accessible via the link <https://www.codegrepper.com/code-examples/python/how+to+get+csv+file+path+in+python>.\n",
        "#Step 3: Convert the column data for \"sentence\" to list data. Note that dataset1_sentences and dataset2_sentences are of the list data type.\n",
        "dataset1_sentences = dataframe_1['sentence'].to_list()\n",
        "dataset2_sentences = dataframe_2['sentence'].to_list()\n",
        "\n",
        "#Step 4: Concatenate the lists together, and output the list data in the form of a descriptive print statement.\n",
        "list_of_sentences = dataset1_sentences + dataset2_sentences\n",
        "\n",
        "# Print the length of list_of_sentences in a f-string formatted text\n",
        "print(f\"The length of list_of_sentences is {len(list_of_sentences)}\")\n",
        "\n",
        "# Remove \"v-noise\" from each sentence string element of the list MandEng_mixed_sentences via a filer-by-regex technique.\n",
        "list_of_sentences_no_vnoice = []\n",
        "\n",
        "for i in range(len(list_of_sentences)):\n",
        "  # reference string: string element of MandEng_mixed_sentences (i.e., MandEng_mixed_sentences[i])\n",
        "  MandEng_mixed_sentences_1 = re.sub(r'<v-noise>', \" \", list_of_sentences[i])\n",
        "  list_of_sentences_no_vnoice.append(MandEng_mixed_sentences_1)\n",
        "\n",
        "# Print the length of list_of_sentences_no_vnoice in a f-string formatted text\n",
        "print(f\"The length of list_of_sentences_no_vnoice is {len(list_of_sentences_no_vnoice)}\")\n",
        "\n",
        "# Verify if the entire dataset contains v-noise \n",
        "for i in range(len(list_of_sentences_no_vnoice)):\n",
        "  if re.search(r'<v-noise>', list_of_sentences_no_vnoice[i]):\n",
        "    print(f\"The sentence {list_of_sentences_no_vnoice[i]} contains <v-noise>\")\n",
        "\n",
        "# Step 5: Split the list_of_sentences_no_vnoice into two different array, one contains only Chinese-major sentences, another contains only English-major sentences.\n",
        "# If the amount of English words reaches 50% of the total amount of words in the sentence, then the sentence is considered to be English-major.\n",
        "english_major = []\n",
        "chinese_major = []\n",
        "\n",
        "for i in range(len(list_of_sentences_no_vnoice)):\n",
        "  # get all of the english words in this sentence\n",
        "  english_words = re.findall(r'[a-zA-Z]+', list_of_sentences_no_vnoice[i])\n",
        "  # get the amount of words in the current sentence\n",
        "  sentence_length = len(list_of_sentences_no_vnoice[i].split())\n",
        "  # If the amount of English words is more than 55% of the total amount of words in the sentence, then the sentence is considered to be English-major.\n",
        "  if len(english_words) / sentence_length >= 0.55:\n",
        "    english_major.append(list_of_sentences_no_vnoice[i])\n",
        "  else:\n",
        "    chinese_major.append(list_of_sentences_no_vnoice[i])\n",
        "  \n",
        "print(f\"There are {len(english_major)} English-major sentences in the dataset.\")\n",
        "print(f\"There are {len(chinese_major)} Chinese-major sentences in the dataset.\")\n",
        "print(f\"Total up to now: {len(english_major) + len(chinese_major)} sentences in the dataset.\")\n",
        "\n",
        "# Write english_major out to dataset_eng_major.txt\n",
        "# with open('dataset_eng_major.txt', 'w') as f:\n",
        "#   for i in range(len(english_major)):\n",
        "#     f.write(english_major[i] + '\\n')\n",
        "\n",
        "# Write chinese_major out to dataset_ch_major.txt\n",
        "# with open('dataset_ch_major.txt', 'w') as f:\n",
        "#   for i in range(len(chinese_major)):\n",
        "#     f.write(chinese_major[i] + '\\n')\n",
        "\n",
        "# Use a smaller dataset for testing, remove this later\n",
        "chinese_major = chinese_major[:100]\n",
        "english_major = english_major[:100]\n"
      ],
      "metadata": {
        "id": "Hidsvq9Ni8o_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5f2de4d-dbd0-494a-db49-a9fb8eace8c8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The length of list_of_sentences is 11850\n",
            "The length of list_of_sentences_no_vnoice is 11850\n",
            "There are 4691 English-major sentences in the dataset.\n",
            "There are 7159 Chinese-major sentences in the dataset.\n",
            "Total up to now: 11850 sentences in the dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code-Switching Boundary Detection , Word Tokenization, and Part of Speech Tagging\n",
        " \t\n",
        "- Initialize a list titled nlp_processed_sentences to contain all the language-compartmentalized, word tokenized, part of speech-tagged sentences. \n",
        "- For each index i in range(len(list_of_sentences)), identify all the boundaries or switching points from English to Mandarin and vice-versa. (Note that we are interested in the part of speech of the word following each switching point.) Then compartmentalize each sentence into all-Mandarin word sequences and all-English word sequences by splitting it at language boundaries before you finally tag the part of speech of every all-Mandarin sequence and tag the part of speech of every all-English sequence."
      ],
      "metadata": {
        "id": "0yHFv70uDkDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MAKE SURE TO FIRST RUN THE FIRST CODE BLOCK AT THE TOP OF THE PAGE\n",
        "# Import of Libraries, Packages, and Modules\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "#Loading of the nlp Machine-Learning Model\n",
        "\n",
        "nlp = spacy.load('zh_core_web_md')\n",
        "\n",
        "def compartmentalization_and_word_tokenization_2(sentence):\n",
        "  \"\"\"\n",
        "Analyzing a single sentence, this function detects the code-switching points,\n",
        "compartmentalizes the sentence into entirely Mandarin fragments and entirely English fragments, \n",
        "word-tokenizes each monolingual fragment, and converts each token to a token-tag tuple. This function mangages the case of identical Mandarin-to-English code-switching points in the same string, such as the repetition of \"个 chocolate\" in '你 要 那 个 chocolate 你 刚 才 吃 的 那 个 chocolate.' We enhanced this function to handle identical English-to-Mandarin code-switching points if time permits. \n",
        "  \"\"\"\n",
        "  #reference string: sentence\n",
        "  Mand_Eng_switches = re.findall(r'[^a-zA-Z]\\s\\b[a-zA-Z]+\\b',sentence)\n",
        "  #return Mand_Eng_switches\n",
        "  #a non-alphabetic character followed by a sequence of alphabetic characters\n",
        "  Eng_Mand_switches = re.findall(r'\\b[a-zA-Z]+\\b\\s[^a-zA-Z]',sentence)\n",
        "  #a sequence of alphabetic characters followed by a non-alphabetic character\n",
        "  ###Insert a comma at each switching point, so initialize a list of indices of positions where commas will be inserted.\n",
        "  comma_indices = []\n",
        "  ##Recover the final-position index of a Mandarin word preceding the switching point.\n",
        "  if len(Mand_Eng_switches) == 0:\n",
        "    pass\n",
        "  elif len(Mand_Eng_switches) == 1:\n",
        "    var1 = Mand_Eng_switches[0]\n",
        "    m_1 = re.search(rf'{var1}',sentence)\n",
        "    position_of_matched_sequence = m_1.span()\n",
        "    comma_index_1 = position_of_matched_sequence[0]+2-1\n",
        "    comma_indices.append(comma_index_1)\n",
        "  else: # Handle identical Mandarin-to-English code-switching points in the same sentence.\n",
        "    sentence_new = sentence[:] #clone the string, so as to not modify the original\n",
        "    for i in range(len(Mand_Eng_switches)):\n",
        "      var1 = Mand_Eng_switches[i]\n",
        "      #m_a = re.search(r'[^a-zA-Z]\\s',item)\n",
        "      #Mandarin_word = m_a.group()\n",
        "      #len(Mandarin_word) would result in 2\n",
        "      #Save the position of the code-switching point to a list.\n",
        "      if i == 0:\n",
        "        m_2 = re.search(rf'{var1}',sentence_new)\n",
        "        position_of_matched_sequence = m_2.span()\n",
        "        comma_index_2 = position_of_matched_sequence[0]+2-1\n",
        "        comma_indices.append(comma_index_2)\n",
        "      if i >= 1:\n",
        "        m_2 = re.search(rf'{var1}',sentence_new)\n",
        "        #return sentence_new\n",
        "        position_of_matched_sequence = m_2.span()\n",
        "        comma_index_2 = len(sentence_new_comp)+position_of_matched_sequence[0]+2-1 #Add the exact length of complementary string because the character count of the complementary substring equals the number by which the index of position_of_matched_sequence[0]+2-1 is offset.\n",
        "        comma_indices.append(comma_index_2)\n",
        "      pstn = int(position_of_matched_sequence[0])\n",
        "      #curtail the part of the sentence before the switching point\n",
        "      sentence_new = sentence_new[pstn+1:]\n",
        "      #Get the complementary string or the portion of the original sentence not in \n",
        "      #the new sentence.\n",
        "      sentence_new_comp = sentence[:pstn+1]\n",
        "  #return comma_indices\n",
        "  ##Recover the final-position index of an English word preceding the switching point.\n",
        "  if len(Eng_Mand_switches) == 0:\n",
        "    pass\n",
        "  elif len(Eng_Mand_switches) == 1:\n",
        "    for item in Eng_Mand_switches:\n",
        "      var3 = item\n",
        "      m_b = re.search(r'[a-zA-Z]+\\s',item)\n",
        "      English_word = m_b.group()\n",
        "      m_3 = re.search(rf'{var3}',sentence)\n",
        "      position_of_matched_sequence = m_3.span()\n",
        "      comma_index_3 = position_of_matched_sequence[0]+len(English_word)-1\n",
        "      comma_indices.append(comma_index_3)\n",
        "    #Attempt to keep the strings the same length by replacing a single space with a       single comma.\n",
        "  else: #Handle identical English-to-Mandarin code-switching points in the same sentence.\n",
        "    sentence_new_1 = sentence[:] #clone the string, so as to not modify the original\n",
        "\n",
        "    #Scour the sentence for all occurrences of a search pattern comprised of an English word, a space, and a Mandarin word.\n",
        "    #reference string: sentence\n",
        "\n",
        "    match_object = re.findall(r'\\b[a-zA-Z]+\\b\\s[^a-zA-Z]',sentence)\n",
        "\n",
        "    if len(match_object) >= 1:\n",
        "      for i in range(len(match_object)):\n",
        "        if i == 0:\n",
        "          #Search for English word in match_object[i] (i.e., the pair of an English and Mandarin word straddling the code-switching point).\n",
        "          m = re.search(r'^[a-zA-Z]+\\b', match_object[i])\n",
        "          Eng_word = m.group()\n",
        "          #print(Eng_word) correct\n",
        "          #Search for match_object[i] in the original sentence.\n",
        "          m_a = re.search(match_object[i],sentence)\n",
        "          indices_of_code_switching_pair = m_a.span()\n",
        "          #Extract the posterior portion of the sentence, and let this override the value of sentence_new_1.\n",
        "          c_s_point = indices_of_code_switching_pair[0]+len(Eng_word) #c_s_point abbreviates code-switching point\n",
        "          sentence_new_1 = sentence_new_1[c_s_point+1:]\n",
        "          #print(f\"portion posterior to code-switching point:{sentence_new_1}\")\n",
        "        if i >= 1:\n",
        "          m = re.search(r'^[a-zA-Z]+\\b', match_object[i])\n",
        "          Eng_word = m.group()\n",
        "          #Search for match_object[i] in the portion posterior to the code-switching point.\n",
        "          m_b = re.search(match_object[i],sentence_new_1)\n",
        "          indices_of_code_switching_pair_post = m_b.span() #indices with respect to the posterior portion\n",
        "          if i == 1:\n",
        "            indices_of_code_switching_pair = tuple((s+len(sentence_new_1_comp) for s in indices_of_code_switching_pair_post))\n",
        "          if i > 1: #I do not understand the reason for the -1 adjustment but figured it out by trial and error.\n",
        "            indices_of_code_switching_pair = tuple((s+len(sentence_new_1_comp)-1 for s in indices_of_code_switching_pair_post))\n",
        "\n",
        "          #Now we have the indices with respect to original sentence in the tuple output from tuple comprehension.\n",
        "          c_s_point = indices_of_code_switching_pair[0]+len(Eng_word)\n",
        "          sentence_new_1 = sentence_new_1[indices_of_code_switching_pair_post[0]+len(Eng_word):] #string from c_s_point with respect to the the posterior portion\n",
        "          #print(f\"portion posterior to code-switching point:{sentence_new_1}\")\n",
        "        sentence_new_1_comp = sentence[:c_s_point+1]\n",
        "        comma_indices.append(c_s_point)   \n",
        "#-----------------\n",
        "  for item in comma_indices:\n",
        "      p = int(item)\n",
        "      sentence = sentence[:p]+','+sentence[p+1:]\n",
        "  compartmentalized_sentence = sentence.split(\",\")\n",
        "  #POS-tag the words of each monolingual fragment \n",
        "  comp_w_t_sent = []\n",
        "  j=0\n",
        "  while j < len(compartmentalized_sentence):\n",
        "    m_5 = re.search(r'[a-z]',compartmentalized_sentence[j])\n",
        "    if m_5 == None: #None, not NONE, is the correct reserved term.\n",
        "        #word tokenize the string titled compartmentalized_sentence[j] with spaCy\n",
        "      #If the monolingual fragment is entirely Mandarin\n",
        "        w_t_str = nlp(compartmentalized_sentence[j])\n",
        "        w_t_str = list(w_t_str)\n",
        "        for i in range(len(w_t_str)): #Guarantee that elements of w_t_str are strings.\n",
        "          w_t_str[i] = str(w_t_str[i])\n",
        "        comp_w_t_sent.append(w_t_str)\n",
        "        j+=1\n",
        "    if type(m_5) == re.Match:\n",
        "      #If the monolingual fragment is entirely English\n",
        "         #word tokenize the string titled compartmentalized_sentence[j] with nltk\n",
        "        w_t_str = nltk.word_tokenize(compartmentalized_sentence[j])\n",
        "        comp_w_t_sent.append(w_t_str)\n",
        "        #comp_w_t_sent stands for compartmentalized, word-tokenized sentence\n",
        "        j+=1\n",
        "  return comp_w_t_sent\n",
        "\n",
        "comparmentalized_chinese_major = []\n",
        "comparmentalized_english_major = []\n",
        "\n",
        "for i in chinese_major:\n",
        "    comparmentalized_chinese_major.append(compartmentalization_and_word_tokenization_2(i))\n",
        "\n",
        "for i in english_major:\n",
        "    comparmentalized_english_major.append(compartmentalization_and_word_tokenization_2(i))\n",
        "\n",
        "# Test print the first 10 lines\n",
        "for i in range(10):\n",
        "    print(comparmentalized_chinese_major[i])\n",
        "\n",
        "for i in range(10):\n",
        "    print(comparmentalized_english_major[i])\n"
      ],
      "metadata": {
        "id": "cAI0C6qPDnIC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b85b23e-d3d6-4037-e252-ab3e54871653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[\"'往\", '下', '一', '点', \"'\"]]\n",
            "[[\"'\", '那', '个'], ['wave'], ['比', '较', '慢', '一', '点', '啊', \"'\"]]\n",
            "[[\"'okay\"], ['可', '以', \"'\"]]\n",
            "[[\"'\", '讲', '多', '一', '点', '咯', '可', '以', \"'\"]]\n",
            "[[\"'okay\"], [' ', \"'\"]]\n",
            "[[\"'okay\"], ['    ', '他', '跟', '你', '讲', '了', '啊', '就', '是', '这', '样', '的', '咯', '  ', '都', '跟', '你', '讲', '这', '是', '这', '样', '了', '啦', '对', '不', '对', \"'\"]]\n",
            "[[\"'\", '哦', '对', '咯', '好', '再', '继', '续', '再', '讲', \"咯'\"]]\n",
            "[[\"'\", '你', '要', '那', '个'], ['chocolate'], ['你', '刚', '才', '吃', '的', '那', '个'], ['chocolate', \"'\"]]\n",
            "[[\"'\", '我', '是', '从'], ['camp'], ['那', '边', '拿', '来', '的', '自', '从'], ['mark'], ['那', '时', '拿', '来', '了', '之', '后', \"'\"]]\n",
            "[[\"'\", '三', '块', '二', \"'\"]]\n",
            "[[\"'yeah\", \"'\"]]\n",
            "[[\"'\"], ['so', 'he', \"'\"]]\n",
            "[[\"'hah\", \"'\"]]\n",
            "[[\"'ha\", \"'\"]]\n",
            "[[\"'ah\", 'hah', 'hah', 'he', 'really', 'did', \"'\"]]\n",
            "[[\"'okay\", \"'\"]]\n",
            "[[\"'\"], ['the', 'knight', 'rider', 'brought', 'down', \"'\"]]\n",
            "[[\"'but\", 'then', 'you', 'did', \"n't\", 'get', 'the', 'rebate', \"'\"]]\n",
            "[[\"'yeah\", 'quite', 'lah', 'sort', 'of', 'lah', 'quite', 'lah', \"'\"]]\n",
            "[[\"'clarke\", 'quay', 'er'], ['怎', '么', '会'], ['bus'], ['会'], ['broke', 'down', \"'\"]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construct a processed sentence (i.e., a word-tokenized and POS-tagged sentence) from list_of_monolingual_sequences with list comprehension and spaCy/nltk."
      ],
      "metadata": {
        "id": "8AwPu7H1EGfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MAKE SURE TO FIRST RUN THE FIRST CODE BLOCK AT THE TOP OF THE PAGE\n",
        "# Importation of modules\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "#Loading of the nlp Machine-Learning Model\n",
        "#Load of the nlp machine-learning model, and allocate it in the variable nlp\n",
        "\n",
        "nlp = spacy.load('zh_core_web_md')\n",
        "\n",
        "\n",
        "\n",
        "def POS_Tagger(comp_w_t_sent):\n",
        "  \"\"\"\n",
        "This function annotates a language-compartmentalized, word-tokenized sentence with abbreviated tags symbolic of lexical categories. In effect, it transforms a sentence list of monolingual fragment lists of tokens into a sentence list of monolingual fragment lists of token-tag tuples.\n",
        "  \"\"\"\n",
        "  annotated_comp_w_t_sent = []\n",
        "  for fragment in comp_w_t_sent:\n",
        "    #reference string: fragment[0]\n",
        "    # Ignore empty fragments\n",
        "    if fragment == []:\n",
        "        continue\n",
        "    m = re.search(r'[a-zA-Z]',fragment[0])\n",
        "    #We do not need to iterate through the token strings of fragment to annotate them.\n",
        "    if m == None:#An initial Mandarin word signals an entirely Mandarin fragment.\n",
        "      #Invoke the nlp model to word-tokenize, and guarantee that the output of the nlp tokenizer is a list.\n",
        "      reassembled_fragment = \" \".join(fragment)\n",
        "      tokens = nlp(reassembled_fragment)\n",
        "      tokens = list(tokens)\n",
        "      tagged_tokens = [(str(s),s.pos_) for s in tokens]\n",
        "        #We could zip a list of tags and a list of tokens into a dictionary and   extract a list of token-tag tuples from the dictionary, though this would ruin the linear order of words.\n",
        "      #Append the list of tag-token tuples to the output list.\n",
        "      annotated_comp_w_t_sent.append(tagged_tokens)\n",
        "    if type(m) == re.Match:\n",
        "      #An initial English word signals an entirely English fragment.\n",
        "      #Call the pos_tag function of nltk. We can forgo the word_tokenize function since the fragment is already word-tokenized.\n",
        "      tagged_tokens = nltk.pos_tag(fragment)\n",
        "      #Append the list of tag-token tuples to the output list.\n",
        "      annotated_comp_w_t_sent.append(tagged_tokens)\n",
        "  return annotated_comp_w_t_sent\n",
        "\n",
        "pos_tagged_ch_major = []\n",
        "pos_tagged_en_major = []\n",
        "\n",
        "for i in comparmentalized_chinese_major:\n",
        "    pos_tagged_ch_major.append(POS_Tagger(i))\n",
        "\n",
        "for i in comparmentalized_english_major:\n",
        "    pos_tagged_en_major.append(POS_Tagger(i))\n",
        "\n",
        "# Test print the first 10 lines\n",
        "for i in range(10):\n",
        "    print(pos_tagged_ch_major[i])\n",
        "\n",
        "for i in range(10):\n",
        "    print(pos_tagged_en_major[i])"
      ],
      "metadata": {
        "id": "FEWUIazEDkhv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3868dc05-6abd-463f-c68c-9ba5007b8a42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[(\"'往\", 'NOUN'), ('下', 'DET'), ('一', 'NUM'), ('点', 'NUM'), (\"'\", 'NUM')]]\n",
            "[[(\"'\", 'VERB'), ('那', 'DET'), ('个', 'NUM')], [('wave', 'NN')], [('比', 'ADP'), ('较', 'ADV'), ('慢', 'VERB'), ('一', 'NUM'), ('点', 'NUM'), ('啊', 'PART'), (\"'\", 'VERB')]]\n",
            "[[(\"'okay\", 'NNS')], [('可', 'VERB'), ('以', 'ADP'), (\"'\", 'VERB')]]\n",
            "[[(\"'\", 'VERB'), ('讲', 'VERB'), ('多', 'ADV'), ('一', 'NUM'), ('点', 'NUM'), ('咯', 'PART'), ('可', 'VERB'), ('以', 'ADP'), (\"'\", 'VERB')]]\n",
            "[[(\"'okay\", 'NNS')], [('  ', 'SPACE'), (\"'\", 'VERB')]]\n",
            "[[(\"'okay\", 'NNS')], [('     ', 'SPACE'), ('他', 'PRON'), ('跟', 'ADP'), ('你', 'PRON'), ('讲', 'VERB'), ('了', 'PART'), ('啊', 'PART'), ('就', 'ADV'), ('是', 'VERB'), ('这', 'DET'), ('样', 'NUM'), ('的', 'PART'), ('咯', 'PART'), ('   ', 'SPACE'), ('都', 'ADV'), ('跟', 'ADP'), ('你', 'PRON'), ('讲', 'VERB'), ('这', 'PRON'), ('是', 'VERB'), ('这', 'DET'), ('样', 'NUM'), ('了', 'PART'), ('啦', 'PART'), ('对', 'VERB'), ('不', 'ADV'), ('对', 'VERB'), (\"'\", 'ADV')]]\n",
            "[[(\"'\", 'VERB'), ('哦', 'PART'), ('对', 'VERB'), ('咯', 'INTJ'), ('好', 'VERB'), ('再', 'ADV'), ('继', 'ADP'), ('续', 'NOUN'), ('再', 'ADV'), ('讲', 'VERB'), (\"咯'\", 'PART')]]\n",
            "[[(\"'\", 'VERB'), ('你', 'PRON'), ('要', 'VERB'), ('那', 'DET'), ('个', 'NUM')], [('chocolate', 'NN')], [('你', 'PRON'), ('刚', 'ADV'), ('才', 'ADV'), ('吃', 'VERB'), ('的', 'PART'), ('那', 'DET'), ('个', 'NUM')], [('chocolate', 'NN'), (\"'\", \"''\")]]\n",
            "[[(\"'\", 'VERB'), ('我', 'PRON'), ('是', 'VERB'), ('从', 'ADP')], [('camp', 'NN')], [('那', 'DET'), ('边', 'ADV'), ('拿', 'VERB'), ('来', 'VERB'), ('的', 'PART'), ('自', 'ADV'), ('从', 'VERB')], [('mark', 'NN')], [('那', 'ADV'), ('时', 'PART'), ('拿', 'VERB'), ('来', 'VERB'), ('了', 'PART'), ('之', 'PART'), ('后', 'NOUN'), (\"'\", 'VERB')]]\n",
            "[[(\"'\", 'VERB'), ('三', 'NUM'), ('块', 'NUM'), ('二', 'NUM'), (\"'\", 'NUM')]]\n",
            "[[(\"'yeah\", 'NNS'), (\"'\", 'POS')]]\n",
            "[[(\"'\", 'VERB')], [('so', 'RB'), ('he', 'PRP'), (\"'\", \"''\")]]\n",
            "[[(\"'hah\", 'NNS'), (\"'\", 'POS')]]\n",
            "[[(\"'ha\", 'NNS'), (\"'\", 'POS')]]\n",
            "[[(\"'ah\", 'POS'), ('hah', 'NN'), ('hah', 'NN'), ('he', 'PRP'), ('really', 'RB'), ('did', 'VBD'), (\"'\", \"''\")]]\n",
            "[[(\"'okay\", 'NNS'), (\"'\", 'POS')]]\n",
            "[[(\"'\", 'VERB')], [('the', 'DT'), ('knight', 'NN'), ('rider', 'NN'), ('brought', 'VBD'), ('down', 'RP'), (\"'\", \"''\")]]\n",
            "[[(\"'but\", 'POS'), ('then', 'RB'), ('you', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('get', 'VB'), ('the', 'DT'), ('rebate', 'NN'), (\"'\", \"''\")]]\n",
            "[[(\"'yeah\", 'CD'), ('quite', 'RB'), ('lah', 'JJ'), ('sort', 'NN'), ('of', 'IN'), ('lah', 'NN'), ('quite', 'RB'), ('lah', 'JJ'), (\"'\", \"''\")]]\n",
            "[[(\"'clarke\", 'POS'), ('quay', 'NN'), ('er', 'NN')], [('怎', 'ADV'), ('么', 'PART'), ('会', 'VERB')], [('bus', 'NN')], [('会', 'VERB')], [('broke', 'VBD'), ('down', 'RP'), (\"'\", \"''\")]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computation of the Number of switched English Nouns, Adjectives, and Verbs"
      ],
      "metadata": {
        "id": "H--JzkU4DkqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def eng_number(annotated_comp_w_t_sent):\n",
        "  \"\"\"\n",
        "Absorbing a part of speech-annotated, language-compartmentalized, and word-tokenized list of lists of tuples as an input, this function totals up the number of switched English fragment-initial nouns, switched English fragment-initial adjectives, and switched English fragment-initial verbs, integrating them into a three-element list output.\n",
        "  \"\"\"\n",
        "  eng_number_values = [0,0,0] #Define all value-storing lists prior to the for loop,   so as to not override them with each iteration of the loop.\n",
        "  # Eliminate the sentence-initial monolingual fragment, which does not follow a code-switching point. \n",
        "  # Do not pop data from the list, that will change the original list and cause error\n",
        "  # annotated_comp_w_t_sent.reverse()\n",
        "  # first_monolingual_segment = annotated_comp_w_t_sent.pop()\n",
        "  # annotated_comp_w_t_sent.reverse()\n",
        "  # Use this code instead\n",
        "  annotated_comp_w_t_sent_new = annotated_comp_w_t_sent[1:]\n",
        "  for fragment in annotated_comp_w_t_sent_new:\n",
        "    #reference string: fragment[0][0]\n",
        "    m = re.search(r'[a-zA-Z]',fragment[0][0]) #0th component of 0th tuple in list\n",
        "    if type(m) == re.Match: #An English first word signals an entirely English fragment.\n",
        "      #Note: lists are mutable, so list indices can be reallocated new values.\n",
        "      if  fragment[0][1] == 'NN': #1st component of 0th tuple in list\n",
        "        eng_number_values[0] = eng_number_values[0]+1\n",
        "      if  fragment[0][1] == 'JJ': #1st component of 0th tuple in list\n",
        "        eng_number_values[1] = eng_number_values[0]+1\n",
        "      if  fragment[0][1] == 'VB' or fragment[0][1] == 'VBP' or fragment[0][1] == 'VBZ' or fragment[0][1] == 'VBN' or fragment[0][1] == 'VBD': #1st component of 0th tuple in list\n",
        "        eng_number_values[2] = eng_number_values[0]+1\n",
        "    else: #A Mandarin first word signals an entirely Mandarin fragment.\n",
        "      pass\n",
        "  return eng_number_values\n",
        "\n",
        "# Three numbers signifies the amount of NOUN, ADJ, VERB\n",
        "eng_number_lst = []\n",
        "\n",
        "# Loop through pos_tagged_en_major\n",
        "for i in pos_tagged_en_major:\n",
        "    temp = eng_number(i)\n",
        "    eng_number_lst.append(temp)\n",
        "    print(f\"{i}: {temp}\")"
      ],
      "metadata": {
        "id": "ZIO7mRIaDk3C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c64a724-119d-4800-b1b8-a4c1712f1446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[(\"'yeah\", 'NNS'), (\"'\", 'POS')]]: [0, 0, 0]\n",
            "[[(\"'\", 'VERB')], [('so', 'RB'), ('he', 'PRP'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'hah\", 'NNS'), (\"'\", 'POS')]]: [0, 0, 0]\n",
            "[[(\"'ha\", 'NNS'), (\"'\", 'POS')]]: [0, 0, 0]\n",
            "[[(\"'ah\", 'POS'), ('hah', 'NN'), ('hah', 'NN'), ('he', 'PRP'), ('really', 'RB'), ('did', 'VBD'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'okay\", 'NNS'), (\"'\", 'POS')]]: [0, 0, 0]\n",
            "[[(\"'\", 'VERB')], [('the', 'DT'), ('knight', 'NN'), ('rider', 'NN'), ('brought', 'VBD'), ('down', 'RP'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'but\", 'POS'), ('then', 'RB'), ('you', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('get', 'VB'), ('the', 'DT'), ('rebate', 'NN'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'yeah\", 'CD'), ('quite', 'RB'), ('lah', 'JJ'), ('sort', 'NN'), ('of', 'IN'), ('lah', 'NN'), ('quite', 'RB'), ('lah', 'JJ'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'clarke\", 'POS'), ('quay', 'NN'), ('er', 'NN')], [('怎', 'ADV'), ('么', 'PART'), ('会', 'VERB')], [('bus', 'NN')], [('会', 'VERB')], [('broke', 'VBD'), ('down', 'RP'), (\"'\", \"''\")]]: [1, 0, 2]\n",
            "[[(\"'it\", 'NN'), ('just', 'RB'), ('broke', 'VBD'), ('down', 'RP'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'who\", 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('someone', 'NN'), (\"'\", 'POS')]]: [0, 0, 0]\n",
            "[[(\"'yeah\", 'NNS'), (\"'\", 'POS')]]: [0, 0, 0]\n",
            "[[(\"'because\", 'IN'), ('i', 'JJ'), ('am', 'VBP'), ('thinking', 'VBG'), ('of', 'IN'), ('it', 'PRP'), ('this', 'DT'), ('way', 'NN'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'the\", 'POS'), ('yeah', 'NN'), ('what', 'WP'), ('happened', 'VBD'), ('to', 'TO'), ('him', 'PRP'), ('hah', 'VB'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'yeah\", 'NNS'), (\"'\", 'POS')]]: [0, 0, 0]\n",
            "[[(\"'russell\", 'NN'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'cause\", 'IN'), ('most', 'JJS'), ('cause', 'VBP'), ('most', 'JJS'), ('of', 'IN'), ('the', 'DT'), ('time', 'NN'), ('say', 'VBP'), ('is', 'VBZ'), ('that', 'IN'), ('new', 'JJ'), ('zealand', 'NN'), ('where', 'WRB'), ('he', 'PRP'), ('got', 'VBD'), ('this', 'DT'), ('nick', 'JJ'), ('name', 'NN'), ('ah', 'NN'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'eh\", 'POS'), ('how', 'WRB'), ('how', 'WRB'), ('many', 'JJ'), ('cups', 'NNS')], [('  ', 'SPACE'), (\"'\", 'VERB')]]: [0, 0, 0]\n",
            "[[(\"'two\", 'CD'), ('jars', 'NNS'), ('yeah', 'RB'), ('two', 'CD'), ('jars', 'NNS'), ('is', 'VBZ'), ('quite', 'RB'), ('a', 'DT'), ('bit', 'NN'), ('lah', 'JJ'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'\", 'VERB'), ('很', 'ADV'), ('死', 'VERB'), ('嘞', 'PART')], [('actually', 'RB'), ('if', 'IN'), ('you', 'PRP'), ('drink', 'VBP'), ('alco', 'VB'), ('if', 'IN'), ('you', 'PRP'), ('drink', 'VBP'), ('beer', 'NN'), ('right', 'NN'), ('it', 'PRP'), ('is', 'VBZ'), ('like', 'IN'), ('for', 'IN'), ('exten', 'NN'), ('extended', 'JJ'), ('period', 'NN'), ('of', 'IN'), ('time', 'NN'), ('right', 'JJ'), ('if', 'IN'), ('let', 'VBN'), ('say', 'VBP'), ('you', 'PRP'), ('get', 'VBP'), ('drunk', 'VB'), ('more', 'RBR'), ('easily', 'RB'), ('then', 'RB'), ('er', 'VB'), ('then', 'RB'), ('if', 'IN'), ('you', 'PRP'), ('yeah', 'VBP'), ('if', 'IN'), ('you', 'PRP'), ('drink', 'VBP'), ('alcohol', 'NNS'), (\"'\", 'POS')]]: [0, 0, 0]\n",
            "[[(\"'not\", \"''\"), ('exactly', 'RB'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'yeah\", 'NNS'), (\"'\", 'POS')]]: [0, 0, 0]\n",
            "[[(\"'um\", 'NNS'), (\"'\", 'POS')]]: [0, 0, 0]\n",
            "[[(\"'eh\", 'NNS'), (\"'\", 'POS')]]: [0, 0, 0]\n",
            "[[(\"'eh\", 'NNS'), (\"'\", 'POS')]]: [0, 0, 0]\n",
            "[[(\"'\", 'VERB')], [('then', 'RB'), ('he', 'PRP'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'\", 'VERB')], [('that', 'DT'), ('so', 'RB'), ('lah', 'VBZ'), ('him', 'PRP')], [('  ', 'SPACE'), (\"'\", 'VERB')]]: [0, 0, 0]\n",
            "[[(\"'然\", 'VERB'), ('后', 'PART'), ('我', 'PRON'), ('等', 'PART')], [('taxi', 'NN'), ('all', 'PDT'), ('the', 'DT'), ('way', 'NN'), ('until', 'IN'), ('e', 'JJ'), ('eleven', 'JJ'), (\"o'clock\", 'NN'), (\"'\", \"''\")]]: [1, 0, 0]\n",
            "[[(\"'it\", 'POS'), (\"'s\", 'POS'), ('like', 'IN'), ('oh', 'NNS'), ('shit', 'VBP')], [('十', 'NUM'), ('一', 'NUM'), ('点', 'NUM'), ('了', 'PART'), (\"'\", 'VERB')]]: [0, 0, 0]\n",
            "[[(\"'hah\", 'NNS'), (\"'\", 'POS')]]: [0, 0, 0]\n",
            "[[(\"'camp\", 'NNS'), (\"'\", 'POS')]]: [0, 0, 0]\n",
            "[[(\"'em\", 'NNS'), (\"'\", 'POS')]]: [0, 0, 0]\n",
            "[[(\"'morning\", 'VBG'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'games\", 'NNS'), ('music', 'NN'), ('live', 'VBP'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'no\", 'NN'), ('i', 'NNS'), ('do', 'VBP'), (\"n't\", 'RB'), ('think', 'VB'), ('it', 'PRP'), ('is', 'VBZ'), ('m', 'JJ'), ('g', 'JJ'), ('l', 'NN'), ('games', 'NNS'), ('music', 'NN'), ('live', 'JJ'), ('yes', 'NNS'), (\"'\", 'POS')]]: [0, 0, 0]\n",
            "[[(\"'yes\", 'NNS'), ('but', 'CC'), ('for', 'IN'), ('goodness', 'JJ'), ('sake', 'NN'), ('they', 'PRP'), ('are', 'VBP'), ('going', 'VBG'), ('from', 'IN'), ('a', 'DT'), ('y', 'NN'), ('e', 'VBZ'), ('a', 'DT'), ('y', 'NN'), ('e', 'NN'), ('here', 'RB'), ('then', 'RB'), ('after', 'IN'), ('that', 'IN'), ('a', 'DT'), ('y', 'NN'), ('e', 'NN')], [('了', 'PART')], [('u', 'JJ'), ('turn', 'NN'), ('back', 'RB'), ('to', 'TO'), ('p', 'VB'), ('i', 'JJ'), ('e', 'NN'), ('then', 'RB'), ('after', 'IN'), ('that', 'DT'), ('go', 'VBP'), ('up', 'RP'), ('to', 'TO'), ('k', 'VB'), ('j', 'NN'), ('e', 'NN'), ('where', 'WRB'), ('krani', 'NN'), ('where', 'WRB'), ('that', 'DT'), ('is', 'VBZ'), ('kallang', 'RB'), ('right', 'JJ'), ('where', 'WRB'), ('if', 'IN'), ('you', 'PRP'), ('turn', 'VBP'), ('left', 'VBN'), ('right', 'NN'), (\"'\", \"''\")]]: [0, 1, 0]\n",
            "[[(\"'ah\", 'NNS'), (\"'\", 'POS')]]: [0, 0, 0]\n",
            "[[(\"'where\", 'POS'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'usually\", 'RB'), ('twelve', 'JJ'), ('thirty', 'NN'), ('by', 'IN'), ('then', 'RB'), ('he', 'PRP'), ('he', 'PRP'), ('shows', 'VBZ'), ('how', 'WRB'), ('long', 'JJ'), ('he', 'PRP'), ('took', 'VBD'), ('the', 'DT'), ('road', 'NN'), ('ah', 'NN'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'\", 'VERB'), ('我', 'PRON'), ('是', 'VERB')], [('like', 'IN'), ('um', 'JJ'), ('i', 'NN'), ('think', 'VBP'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'no\", 'IN'), ('he', 'PRP'), ('can', 'MD'), ('go', 'VB'), ('through', 'IN'), ('those', 'DT'), ('small', 'JJ'), ('roads', 'NNS'), ('then', 'RB'), ('after', 'IN'), ('that', 'DT'), ('go', 'VBP'), ('down', 'RP'), ('then', 'RB'), ('after', 'IN'), ('that', 'DT')], [('我', 'PRON'), ('在', 'ADV'), ('想', 'VERB'), ('不', 'ADV'), ('用', 'VERB'), ('紧', 'VERB'), ('啦', 'PART')], [('i', 'JJ'), ('think', 'VBP'), ('i', 'NNS'), ('leave', 'VBP'), ('it', 'PRP'), ('to', 'TO'), ('him', 'PRP'), ('first', 'RB')], [('啦', 'PART')], [('because', 'IN'), ('i', 'JJ'), ('think', 'VBP'), ('he', 'PRP'), ('i', 'VBZ'), ('think', 'VBP'), ('he', 'PRP'), ('has', 'VBZ'), ('missed', 'VBN'), ('that', 'IN'), ('p', 'NN'), ('i', 'NN'), ('e', 'VBP'), ('turn', 'NN'), (\"'\", \"''\")]]: [0, 1, 0]\n",
            "[[(\"'provided\", 'VBN'), ('you', 'PRP'), ('are', 'VBP'), ('at', 'IN'), ('kallang', 'NNS'), ('so', 'RB'), ('you', 'PRP'), ('have', 'VBP'), ('to', 'TO'), ('go', 'VB'), ('through', 'IN'), ('k', 'NN'), ('p', 'NN'), ('e', 'NN'), ('first', 'RB'), ('before', 'IN'), ('you', 'PRP'), ('can', 'MD'), ('go', 'VB'), ('through', 'IN'), ('s', 'JJ'), ('l', 'NN'), ('before', 'IN'), ('you', 'PRP'), ('can', 'MD'), ('go', 'VB'), ('er', 'JJ'), ('kallang', 'NN'), ('payar', 'NN'), ('lebar', 'NN'), ('expressway', 'NN')], [('的', 'PART')], [('underneath', 'JJ'), ('yeah', 'UH'), ('yes', 'NNS'), ('but', 'CC'), ('pa', 'NN'), (\"'\", \"''\")]]: [0, 1, 0]\n",
            "[[(\"'ah\", 'CD'), ('no', 'DT'), ('seventy', 'NN'), ('k', 'NN'), ('but', 'CC'), ('then', 'RB'), ('the', 'DT'), ('thing', 'NN'), ('is', 'VBZ'), ('that', 'IN'), ('that', 'DT'), ('time', 'NN'), ('the', 'DT'), ('stretch', 'NN'), ('where', 'WRB'), ('the', 'DT'), ('stretch', 'NN'), ('from', 'IN'), ('p', 'NN'), ('i', 'NN'), ('e', 'VBP'), ('to', 'TO'), ('s', 'VB'), ('l', 'NN'), ('e', 'NN'), ('has', 'VBZ'), ('not', 'RB'), ('been', 'VBN'), ('eh', 'JJ'), ('no', 'DT'), ('no', 'DT'), ('no', 'DT'), ('which', 'WDT'), ('stretch', 'VBP'), ('oh', 'IN'), ('the', 'DT'), ('stretch', 'NN'), ('from', 'IN'), ('p', 'NN'), ('i', 'NN'), ('e', 'VBP'), ('to', 'TO'), ('s', 'VB'), ('l', 'NN'), ('e', 'NN'), ('has', 'VBZ'), ('not', 'RB'), ('been', 'VBN'), ('has', 'VBZ'), ('not', 'RB'), ('been', 'VBN'), ('done', 'VBN'), ('so', 'RB'), ('is', 'VBZ'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'to\", 'POS'), ('west', 'JJS'), ('side', 'NN'), ('the', 'DT'), ('east', 'JJ'), ('side', 'NN'), ('the', 'DT'), ('east', 'JJ'), ('side', 'NN'), ('p', 'NN'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'ah\", 'CD'), ('ha', 'NN'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'hu\", 'POS'), ('oh', 'PRP'), ('yeah', 'VBP'), ('never', 'RB'), ('thought', 'VBN'), ('of', 'IN'), ('that', 'DT'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'yes\", 'NNS'), (\"'\", 'POS')]]: [0, 0, 0]\n",
            "[[(\"'ah\", 'IN'), ('you', 'PRP'), ('have', 'VBP'), ('you', 'PRP'), ('ever', 'RB'), ('seen', 'VBN'), ('taxi', 'NN'), ('driver', 'NN'), ('reach', 'NN'), ('hundred', 'CD'), ('and', 'CC'), ('forty', 'VB'), ('hundred', 'CD'), ('and', 'CC'), ('sixty', 'NNS'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'\", 'VERB')], [('i', 'NN'), ('paid', 'VBD'), ('less', 'RBR'), ('when', 'WRB'), ('he', 'PRP'), ('travelled', 'VBD'), ('when', 'WRB'), ('he', 'PRP'), ('travelled', 'VBD'), ('faster', 'RBR'), ('yeah', 'NN'), (\"'\", \"''\")]]: [1, 0, 0]\n",
            "[[(\"'em\", 'POS'), ('forty', 'NN'), ('cents', 'NNS'), ('difference', 'NN'), ('ma', 'NN'), ('as', 'IN'), ('well', 'RB'), ('take', 'VB'), ('the', 'DT'), ('forty', 'NN'), ('cents', 'NNS'), ('lah', 'RB'), ('right', 'RB'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'he\", 'NNS'), ('can', 'MD'), ('he', 'PRP'), ('already', 'RB'), ('see', 'VB'), ('him', 'PRP'), ('already', 'RB'), ('see', 'VBP'), ('him', 'PRP'), ('pushing', 'VBG'), ('all', 'PDT'), ('the', 'DT'), ('way', 'NN'), ('to', 'TO'), ('hundred', 'CD'), ('and', 'CC'), ('eighty', 'VB'), ('on', 'IN'), ('that', 'DT'), ('on', 'IN'), ('that', 'DT'), ('on', 'IN'), ('that', 'DT'), ('t', 'NN'), ('p', 'NN'), ('e', 'NN'), ('straight', 'VBD'), ('road', 'NN'), ('lah', 'NN'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'yeah\", 'POS'), ('t', 'NN'), ('p', 'NN'), ('e', 'NN'), ('can', 'MD'), ('push', 'VB'), ('s', 'JJ'), ('l', 'NN'), ('e', 'NN'), ('i', 'NN'), ('think', 'VBP'), ('he', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('because', 'IN'), ('er', 'VB'), ('he', 'PRP'), ('might', 'MD'), ('he', 'PRP'), ('was', 'VBD'), ('thinking', 'VBG'), ('hah', 'NN'), ('i', 'NN'), ('might', 'MD'), ('complaint', 'VB'), ('after', 'IN'), ('that', 'DT'), ('he', 'PRP'), ('saw', 'VBD'), ('me', 'PRP'), ('it', 'PRP'), ('is', 'VBZ'), ('like', 'IN'), ('hm', 'NN'), ('okay', 'NN'), ('lah', 'NN'), ('nothing', 'NN'), ('right', 'JJ'), ('okay', 'NN'), ('nothing', 'NN'), ('right', 'RB'), ('he', 'PRP'), ('reach', 'VB'), ('hundred', 'CD'), ('and', 'CC'), ('twenty', 'VB'), ('he', 'PRP'), ('is', 'VBZ'), ('a', 'DT'), ('old', 'JJ'), ('man', 'NN'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'hm\", 'POS'), ('i', 'NN'), ('know', 'VBP'), ('it', 'PRP'), ('is', 'VBZ'), ('very', 'RB'), ('flexible', 'JJ'), ('but', 'CC'), ('i', 'JJ'), ('see', 'VBP'), ('him', 'PRP'), ('quite', 'RB'), ('young', 'JJ'), ('it', 'PRP'), ('does', 'VBZ'), (\"n't\", 'RB'), ('and', 'CC'), ('i', 'VB'), ('see', 'VBP'), ('he', 'PRP'), ('is', 'VBZ'), ('the', 'DT'), ('type', 'NN'), ('he', 'PRP'), ('wo', 'MD'), (\"n't\", 'RB'), ('rush', 'VB'), ('it', 'PRP'), ('do', 'VBP'), (\"n't\", 'RB'), ('think', 'VB'), ('so', 'IN'), ('it', 'PRP'), ('is', 'VBZ'), ('like', 'IN'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'\", 'VERB')], [('eh', 'VB'), ('the', 'DT'), ('hah', 'NN'), (\"'\", 'POS')]]: [0, 0, 1]\n",
            "[[(\"'i\", 'MD'), ('think', 'VB'), ('fort', 'NN'), ('yeah', 'PRP'), ('forty', 'CD'), ('years', 'NNS')], [('新', 'ADV'), ('加', 'VERB'), ('坡', 'NOUN'), ('人', 'NOUN'), (\"'\", 'VERB')]]: [0, 0, 0]\n",
            "[[(\"'yes\", 'NNS'), ('holiday', 'NN'), ('holiday', 'NN')], [('想', 'VERB'), ('要', 'VERB'), ('去', 'VERB')], [('apply', 'NN'), ('for', 'IN'), ('job', 'NN'), ('but', 'CC'), ('apply', 'VB')], [('不', 'ADV'), ('到', 'VERB'), (\"'\", 'NOUN')]]: [1, 0, 0]\n",
            "[[(\"'it\", 'CD'), ('seriously', 'RB'), ('lah', 'IN'), ('it', 'PRP'), ('is', 'VBZ'), ('like', 'IN'), ('okay', 'NN'), ('i', 'NN'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'eh\", 'NNS'), (\"'\", 'POS')]]: [0, 0, 0]\n",
            "[[(\"'kana\", 'POS'), ('kua', 'NN'), ('kana', 'NN'), ('buei', 'NN'), ('hiao', 'NN'), ('tia', 'NN'), ('ah', 'NN'), ('hah', 'NN'), ('angmo', 'IN'), ('leh', 'NN'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'i\", 'NNS'), ('give', 'VBP'), ('up', 'RP')], [('  ', 'SPACE'), (\"'\", 'VERB')]]: [0, 0, 0]\n",
            "[[(\"'i\", 'POS'), ('am', 'VBP'), ('teochew', 'NN'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'yes\", 'NNS'), ('er', 'VBP')], [(' ', 'SPACE')], [('not', 'RB'), ('that', 'IN'), ('as', 'RB'), ('fluent', 'JJ'), ('not', 'RB'), ('that', 'IN'), ('as', 'IN'), ('fluent', 'JJ'), ('lah', 'NN'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'oh\", 'POS'), ('yeah', 'NN'), ('ehsai', 'NN'), ('ah', 'NN'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'buehsai\", 'CD'), ('ah', 'NN'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'that\", 'DT'), ('is', 'VBZ'), ('also', 'RB'), ('teochew', 'JJ'), ('ehsai', 'RB'), ('buehsai', 'NN'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'buehsai\", 'POS'), ('lah', 'NN'), ('mai', 'NN'), ('ta', 'NN'), ('lah', 'NN'), ('aiya', 'NN'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'eh\", 'CD'), ('wa', 'NN'), ('ai', 'NN'), ('ker', 'NN'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'wa\", 'POS'), ('ai', 'NN'), ('ker', 'NN'), ('i', 'NN'), ('want', 'VBP'), ('to', 'TO'), ('go', 'VB')], [('我', 'PRON'), ('要', 'VERB'), ('去', 'VERB'), (\"'\", 'VERB')]]: [0, 0, 0]\n",
            "[[(\"'\", 'VERB'), ('那', 'DET'), ('时', 'PART'), ('候', 'VERB'), ('我', 'PRON'), ('直', 'ADV'), ('接', 'VERB'), ('这', 'DET'), ('样', 'NUM'), ('我', 'PRON'), ('好', 'ADV'), ('像', 'VERB'), ('是', 'VERB'), ('啊', 'PART')], [('just', 'RB'), ('speak', 'VB'), ('straight', 'JJ'), ('to', 'TO'), ('the', 'DT'), ('point', 'NN'), ('lah', 'NN'), ('like', 'IN'), ('what', 'WP'), ('i', 'NN'), ('want', 'VBP'), ('to', 'TO'), ('do', 'VB'), ('lah', 'VB'), ('wa', 'VB'), ('ai', 'JJ'), ('ker', 'NN')], [('   ', 'SPACE')], [('do', 'VB'), ('my', 'PRP$'), ('number', 'NN'), ('my', 'PRP$'), ('my', 'PRP$'), ('number', 'NN'), ('one', 'CD'), ('and', 'CC'), ('my', 'PRP$'), ('number', 'NN'), ('two', 'CD'), ('lah', 'NN'), ('yeah', 'NN')], [('  ', 'SPACE'), (\"'\", 'VERB')]]: [0, 0, 1]\n",
            "[[(\"'yeah\", 'NNS'), (\"'\", 'POS')]]: [0, 0, 0]\n",
            "[[(\"'\", 'VERB')], [('in', 'IN'), ('french', 'JJ'), ('ah', 'NN'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'yeah\", 'NNS'), (\"'\", 'POS')]]: [0, 0, 0]\n",
            "[[(\"'ah\", 'CD'), ('ha', 'NN'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'it\", \"''\"), ('is', 'VBZ'), ('like', 'IN'), ('then', 'RB'), ('after', 'IN'), ('that', 'DT'), ('what', 'WP'), ('they', 'PRP'), ('what', 'WP'), ('they', 'PRP'), ('say', 'VBP'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'french\", 'NN'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'yes\", 'NNS'), (\"'\", 'POS')]]: [0, 0, 0]\n",
            "[[(\"'er\", 'CD'), ('no', 'DT'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'yeah\", 'NNS'), (\"'\", 'POS')]]: [0, 0, 0]\n",
            "[[(\"'yeah\", 'NNS'), (\"'\", 'POS')]]: [0, 0, 0]\n",
            "[[(\"'er\", 'NNS'), (\"'\", 'POS')]]: [0, 0, 0]\n",
            "[[(\"'hah\", 'NNS'), (\"'\", 'POS')]]: [0, 0, 0]\n",
            "[[(\"'ah\", 'NNS'), (\"'\", 'POS')]]: [0, 0, 0]\n",
            "[[(\"'um\", 'NNS'), (\"'\", 'POS')]]: [0, 0, 0]\n",
            "[[(\"'yeah\", 'CD'), ('yeah', 'NN'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'laugh\", 'IN'), ('ah', 'NN')], [(' ', 'SPACE')], [('laugh', 'RB'), ('good', 'JJ'), ('day', 'NN'), ('yeah', 'VB')], [('对', 'VERB')], [('okay', 'NN'), (\"'\", \"''\")]]: [1, 0, 0]\n",
            "[[(\"'er\", 'NNS'), (\"'\", 'POS')]]: [0, 0, 0]\n",
            "[[(\"'er\", 'NNS'), (\"'\", 'POS')]]: [0, 0, 0]\n",
            "[[(\"'er\", 'NNS'), (\"'\", 'POS')]]: [0, 0, 0]\n",
            "[[(\"'\", 'VERB')], [('okay', 'JJ'), ('okay', 'MD'), ('okay', 'VB'), (\"'\", \"''\")]]: [0, 1, 0]\n",
            "[[(\"'yes\", 'CD'), ('she', 'PRP'), ('is', 'VBZ'), ('more', 'JJR'), ('of', 'IN'), ('a', 'DT'), ('traditionist', 'NN'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'that\", 'DT'), ('is', 'VBZ'), ('like', 'IN'), ('out', 'IN'), ('of', 'IN'), ('all', 'DT'), ('sudden', 'JJ')], [('  ', 'SPACE'), (\"'\", 'VERB')]]: [0, 0, 0]\n",
            "[[(\"'you\", 'NNS'), ('said', 'VBD'), ('you', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('talk', 'VB'), ('about', 'IN'), ('computer', 'NN'), ('right', 'NN'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'the\", 'CD'), ('speaker', 'NN'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'really\", 'RB'), ('good', 'JJ'), ('is', 'VBZ'), ('it', 'PRP'), ('good', 'JJ'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'you\", 'CD'), ('got', 'VBD'), ('it', 'PRP'), ('as', 'IN'), ('a', 'DT'), ('present', 'JJ'), ('or', 'CC'), ('you', 'PRP'), ('brought', 'VBD'), ('it', 'PRP'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'forty\", 'CD'), ('dollars', 'NNS'), (\"'\", 'POS')]]: [0, 0, 0]\n",
            "[[(\"'you\", 'IN'), ('you', 'PRP'), ('heard', 'VBP'), ('it', 'PRP'), ('just', 'RB'), ('now', 'RB'), ('yes', 'RB'), ('so', 'RB'), ('quite', 'RB'), ('okay', 'JJ'), ('but', 'CC'), ('then', 'RB'), ('do', 'VBP'), (\"n't\", 'RB'), ('expect', 'VB'), ('a', 'DT'), ('lot', 'NN'), ('lah', 'NN'), ('from', 'IN'), ('this', 'DT'), ('lah', 'NN'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'one\", 'CD'), ('piece', 'NN'), ('one', 'CD'), ('small', 'JJ'), ('little', 'JJ'), ('piece', 'NN'), ('of', 'IN'), ('something', 'NN'), ('like', 'IN'), ('this', 'DT'), ('yeah', 'NN'), ('eh', 'VB'), ('your', 'PRP$'), ('your', 'PRP$'), ('are', 'VBP'), ('you', 'PRP'), ('planning', 'VBG'), ('for', 'IN'), ('your', 'PRP$'), ('next', 'JJ'), ('semester', 'NN'), ('course', 'NN'), ('or', 'CC'), ('are', 'VBP'), ('you', 'PRP'), ('still', 'RB'), (\"'\", \"''\")]]: [0, 0, 0]\n",
            "[[(\"'because\", 'IN'), ('no', 'DT'), ('no', 'DT'), ('no', 'DT'), ('i', 'JJ'), ('mean', 'VBP'), ('because', 'IN'), ('you', 'PRP'), ('i', 'VBP'), ('already', 'RB'), ('received', 'VBN'), ('a', 'DT'), ('letter', 'NN'), ('stating', 'VBG'), ('that', 'IN'), ('i', 'JJ'), ('need', 'VBP'), ('to', 'TO'), ('plan', 'VB'), ('for', 'IN'), ('my', 'PRP$'), ('next', 'JJ'), ('for', 'IN'), ('my', 'PRP$'), ('courses', 'NNS'), ('next', 'IN'), ('year', 'NN'), ('it', 'PRP'), ('is', 'VBZ'), ('like', 'IN'), ('how', 'WRB'), ('to', 'TO'), ('plan', 'VB'), ('for', 'IN'), ('plan', 'NN'), ('for', 'IN'), ('my', 'PRP$'), ('timetable', 'JJ'), ('next', 'JJ'), ('year', 'NN'), ('already', 'RB'), ('so', 'IN'), ('you', 'PRP'), (\"'\", \"''\")]]: [0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computation of the Number of switched Mandarin Nouns, Adjectives, and Verbs"
      ],
      "metadata": {
        "id": "dp8vsuT_DlAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chin_number(annotated_comp_w_t_sent):\n",
        "  \"\"\"\n",
        "Absorbing a part of speech-annotated, language-compartmentalized, and word-tokenized list of lists of tuples as an input, this function totals up the number of switched English fragment-initial nouns, switched English fragment-initial adjectives, and switched English fragment-initial verbs, integrating them into a three-element list output.\n",
        "  \"\"\"\n",
        "  #Eliminate the sentence-initial monolingual fragment, which does not follow a code-switching point.\n",
        "  annotated_comp_w_t_sent.reverse()\n",
        "  first_monolingual_segment = annotated_comp_w_t_sent.pop() \n",
        "  annotated_comp_w_t_sent.reverse()\n",
        "  chin_number_values = [0,0,0] #Define all value-storing lists prior to the for loop, so as to not override them with each iteration of the loop.\n",
        "  for fragment in annotated_comp_w_t_sent:\n",
        "    #reference string: fragment[0]\n",
        "    m = re.search(r'[a-zA-Z]',fragment[0][0])\n",
        "    if m == None: #A Mandarin first word signals an entirely Mandarin fragment.\n",
        "      #Note: lists are mutable, so list indices can be reallocated new values.\n",
        "      if  fragment[0][1] == 'NOUN':\n",
        "        chin_number_values[0]+=1\n",
        "      if  fragment[0][1] == 'ADJ':\n",
        "        chin_number_values[1]+=1\n",
        "      if  fragment[0][1] == 'VERB':\n",
        "        chin_number_values[2]+=1\n",
        "    else: #An English first word signals an entirely English fragment.\n",
        "      pass\n",
        "  return chin_number_values\n",
        "\n",
        "print(chin_number([[('你', 'PRON'), ('要', 'VERB'), ('那', 'DET'), ('个', 'NUM')], [('chocolate', 'NN')], [('你', 'PRON'), ('刚', 'ADV'), ('才', 'ADV'), ('吃', 'VERB'), ('的', 'PART'), ('那', 'DET'), ('个', 'NUM')], [('chocolate', 'NN')], [('你', 'PRON')]]))\n",
        "print(chin_number([[('我', 'PRON'), ('是', 'VERB'), ('从', 'ADP')], [('camp', 'NN')], [('那', 'DET'), ('边', 'ADV'), ('拿', 'VERB'), ('来', 'VERB'), ('的', 'PART'), ('自', 'ADV'), ('从', 'VERB')], [('mark', 'NN')], [('那', 'ADV'), ('时', 'PART'), ('拿', 'VERB'), ('来', 'VERB'), ('了', 'PART'), ('之', 'PART'), ('后', 'NOUN')]]))"
      ],
      "metadata": {
        "id": "a0tdTRlmDlUl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6b2cb15-a83e-4a9d-b138-000ba8473505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0]\n",
            "[0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a bar chart or other diagram in excel, colab, etc that can visually illustrate the number of switched words that are adjectives, nouns, and verbs and the number of switched words that are adjectives, nouns, and verbs.\n",
        "\n"
      ],
      "metadata": {
        "id": "wzLfoqGfDlcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "0XVXp-LB2r8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bar_chart(eng_number, chin_number):\n",
        "    left = np.array([1, 2])\n",
        "    labels = [\"English\", \"Chinese\"]\n",
        "    nouns_bar = np.array([eng_number[0], chin_number[0]])\n",
        "    adjs_bar = np.array([eng_number[1], chin_number[1]])\n",
        "    verbs_bar = np.array([eng_number[2], chin_number[2]])\n",
        "    p1 = plt.bar(left, nouns_bar, color=\"green\", tick_label = labels)\n",
        "    p2 = plt.bar(left, adjs_bar, bottom=nouns_bar, color=\"orange\")\n",
        "    p3 = plt.bar(left, verbs_bar, bottom=adjs_bar+nouns_bar, color=\"red\")\n",
        "    plt.legend((p1[0], p2[0], p3[0]), (\"NOUN / NN\", \"ADJ / JJ\", \"VERB / VB\"))\n",
        "    plt.ylabel(\"Number of POS\")\n",
        "    plt.title(\"The numbers of each POS in English and Chinese\")\n",
        "    plt.show()\n",
        "\n",
        "bar_chart([200,300,400],[100,200,300])"
      ],
      "metadata": {
        "id": "iA4niW3tEnbl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "702d2e58-21a7-4c76-fa09-001534fbb4c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}