{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Installation of files and libraries\n",
        "- Install spacy and nltk"
      ],
      "metadata": {
        "id": "IxqTe-con3pV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Installation of Libraries and Downloadind of nlp Machine-Learning Model\n",
        "\n",
        "!pip install pandas\n",
        "!pip install -U spacy\n",
        "\n",
        "!python -m spacy download zh_core_web_md\n",
        "\n",
        "# Move the import statement up here so that it doesn't reinstalled everything everytime a code block runs\n",
        "import nltk\n",
        "nltk.download('popular')\n",
        "nltk.download('tagsets')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5_MddaToAcm",
        "outputId": "bcd46d94-4875-4d38-c8fd-2d02503532df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.2.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 3.7 MB/s \n",
            "\u001b[?25hCollecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n",
            "\u001b[K     |████████████████████████████████| 451 kB 58.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.63.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.0)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
            "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 32.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.10.0.2)\n",
            "Collecting thinc<8.1.0,>=8.0.12\n",
            "  Downloading thinc-8.0.13-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (628 kB)\n",
            "\u001b[K     |████████████████████████████████| 628 kB 51.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 59.6 MB/s \n",
            "\u001b[?25hCollecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.6 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 spacy-3.2.3 spacy-legacy-3.0.9 spacy-loggers-1.0.1 srsly-2.4.2 thinc-8.0.13 typer-0.4.0\n",
            "Collecting zh-core-web-md==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/zh_core_web_md-3.2.0/zh_core_web_md-3.2.0-py3-none-any.whl (79.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 79.0 MB 63 kB/s \n",
            "\u001b[?25hCollecting spacy-pkuseg<0.1.0,>=0.0.27\n",
            "  Downloading spacy_pkuseg-0.0.28-cp37-cp37m-manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 2.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from zh-core-web-md==3.2.0) (3.2.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (0.9.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (21.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (1.21.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (2.4.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (1.0.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (3.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (57.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (4.63.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (0.4.0)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (8.0.13)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (3.0.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (2.10)\n",
            "Requirement already satisfied: cython>=0.25 in /usr/local/lib/python3.7/dist-packages (from spacy-pkuseg<0.1.0,>=0.0.27->zh-core-web-md==3.2.0) (0.29.28)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->zh-core-web-md==3.2.0) (2.0.1)\n",
            "Installing collected packages: spacy-pkuseg, zh-core-web-md\n",
            "Successfully installed spacy-pkuseg-0.0.28 zh-core-web-md-3.2.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('zh_core_web_md')\n",
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw-1.4.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2021.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet31.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Step\n",
        "\n",
        "- Read the column titled “sentence” in dataset1.txt as well as in dataset2.txt into a single list_of_sentences.\n",
        "- Filter out \"v-noise\" from the sentence string elements of the list\n",
        "- Split list_of_sentences into two different array, one contains only Chinese-major sentences, another contains only English-major sentences.\n"
      ],
      "metadata": {
        "id": "SGSHkaZPDjvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing Step\n",
        "\n",
        "# - Read the column titled “sentence” in dataset1.csv as well as in dataset2.csv into a single list_of_sentences.\n",
        "# - Filter out \"v-noise\" from the sentence string elements of the list\n",
        "# - Split list_of_sentences into two different array, one contains only Chinese-major sentences, another contains only English-major sentences.\n",
        "# - Output both array as two seperate datasets titled dataset_eng_major.txt and dataset_ch_major.txt\n",
        "\n",
        "from pandas import *\n",
        "import re \n",
        "\n",
        "#Step 2: read the two csv files into two dataframes by means of the read_csv function\n",
        "# Use a direct filepath so that when datasets are imported, it won't error\n",
        "dataframe_1 = read_csv('dataset1.csv') # DONT CHANGE THESE!\n",
        "dataframe_2 = read_csv('dataset2.csv')\n",
        " \n",
        "#We are reading csv files, not txt files, so we do not employ the open function above, but we can specify the file path in the read_csv function \n",
        "#just as we would in the open function, according to the code snipped accessible via the link <https://www.codegrepper.com/code-examples/python/how+to+get+csv+file+path+in+python>.\n",
        "#Step 3: Convert the column data for \"sentence\" to list data. Note that dataset1_sentences and dataset2_sentences are of the list data type.\n",
        "dataset1_sentences = dataframe_1['sentence'].to_list()\n",
        "dataset2_sentences = dataframe_2['sentence'].to_list()\n",
        "\n",
        "#Step 4: Concatenate the lists together, and output the list data in the form of a descriptive print statement.\n",
        "list_of_sentences = dataset1_sentences + dataset2_sentences\n",
        "\n",
        "# Print the length of list_of_sentences in a f-string formatted text\n",
        "print(f\"The length of list_of_sentences is {len(list_of_sentences)}\")\n",
        "\n",
        "# Remove \"v-noise\" from each sentence string element of the list MandEng_mixed_sentences via a filer-by-regex technique.\n",
        "list_of_sentences_no_vnoice = []\n",
        "\n",
        "for i in range(len(list_of_sentences)):\n",
        "  # reference string: string element of MandEng_mixed_sentences (i.e., MandEng_mixed_sentences[i])\n",
        "  MandEng_mixed_sentences_1 = re.sub(r'<v-noise>', \" \", list_of_sentences[i])\n",
        "  list_of_sentences_no_vnoice.append(MandEng_mixed_sentences_1)\n",
        "\n",
        "# Print the length of list_of_sentences_no_vnoice in a f-string formatted text\n",
        "print(f\"The length of list_of_sentences_no_vnoice is {len(list_of_sentences_no_vnoice)}\")\n",
        "\n",
        "# Verify if the entire dataset contains v-noise \n",
        "for i in range(len(list_of_sentences_no_vnoice)):\n",
        "  if re.search(r'<v-noise>', list_of_sentences_no_vnoice[i]):\n",
        "    print(f\"The sentence {list_of_sentences_no_vnoice[i]} contains <v-noise>\")\n",
        "\n",
        "# Step 5: Split the list_of_sentences_no_vnoice into two different array, one contains only Chinese-major sentences, another contains only English-major sentences.\n",
        "# If the amount of English words reaches 50% of the total amount of words in the sentence, then the sentence is considered to be English-major.\n",
        "english_major = []\n",
        "chinese_major = []\n",
        "\n",
        "for i in range(len(list_of_sentences_no_vnoice)):\n",
        "  # get all of the english words in this sentence\n",
        "  english_words = re.findall(r'[a-zA-Z]+', list_of_sentences_no_vnoice[i])\n",
        "  # get the amount of words in the current sentence\n",
        "  sentence_length = len(list_of_sentences_no_vnoice[i].split())\n",
        "  # If the amount of English words is more than 55% of the total amount of words in the sentence, then the sentence is considered to be English-major.\n",
        "  if len(english_words) / sentence_length >= 0.55:\n",
        "    english_major.append(list_of_sentences_no_vnoice[i][1:-1])    #Slicing from [1:-1] removes quotation marks from sentences because quotation marks are string-initial and string-final.\n",
        "  else:\n",
        "    chinese_major.append(list_of_sentences_no_vnoice[i][1:-1])    #Slicing from [1:-1] removes quotation marks from sentences because quotation marks are string-initial and string-final.\n",
        "  \n",
        "print(f\"There are {len(english_major)} English-major sentences in the dataset.\")\n",
        "print(f\"There are {len(chinese_major)} Chinese-major sentences in the dataset.\")\n",
        "print(f\"Total up to now: {len(english_major) + len(chinese_major)} sentences in the dataset.\")"
      ],
      "metadata": {
        "id": "Hidsvq9Ni8o_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21a85d28-a92d-4c9c-f7ad-19fb1fd4ff96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The length of list_of_sentences is 11850\n",
            "The length of list_of_sentences_no_vnoice is 11850\n",
            "There are 4691 English-major sentences in the dataset.\n",
            "There are 7159 Chinese-major sentences in the dataset.\n",
            "Total up to now: 11850 sentences in the dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code-Switching Boundary Detection , Word Tokenization, and Part of Speech Tagging\n",
        " \t\n",
        "- Initialize a list titled nlp_processed_sentences to contain all the language-compartmentalized, word tokenized, part of speech-tagged sentences. \n",
        "- For each index i in range(len(list_of_sentences)), identify all the boundaries or switching points from English to Mandarin and vice-versa. (Note that we are interested in the part of speech of the word following each switching point.) Then compartmentalize each sentence into all-Mandarin word sequences and all-English word sequences by splitting it at language boundaries before you finally tag the part of speech of every all-Mandarin sequence and tag the part of speech of every all-English sequence."
      ],
      "metadata": {
        "id": "0yHFv70uDkDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MAKE SURE TO FIRST RUN THE FIRST CODE BLOCK AT THE TOP OF THE PAGE\n",
        "# Import of Libraries, Packages, and Modules\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "#Loading of the nlp Machine-Learning Model\n",
        "\n",
        "nlp = spacy.load('zh_core_web_md')\n",
        "\n",
        "def compartmentalization_and_word_tokenization_2(sentence):\n",
        "    \"\"\"\n",
        "    Return a list of list of the compartmentalized sentence\n",
        "\n",
        "    sentence: The input code-switched sentence to be compartmentalized\n",
        "    \"\"\"\n",
        "    sentence_lst = sentence.split()\n",
        "    sentence_new = [[]]\n",
        "    compartments = 0\n",
        "    last_is_chinese = False\n",
        "    for i in sentence_lst:\n",
        "        # If last is chinese but current is english, its a switch\n",
        "        if re.search(\"[a-zA-Z]+\", i) and last_is_chinese:\n",
        "            sentence_new.append([])\n",
        "            compartments += 1\n",
        "            sentence_new[compartments].append(i)\n",
        "            last_is_chinese = False\n",
        "        # If last is not chinese and current is chinese, its a switch\n",
        "        elif not re.search(\"[a-zA-Z]+\", i) and not last_is_chinese:\n",
        "            sentence_new.append([])\n",
        "            compartments += 1\n",
        "            sentence_new[compartments].append(i)\n",
        "            last_is_chinese = True\n",
        "        # If last is chinese and current is chinese, continue with the current compartment\n",
        "        elif not re.search(\"[a-zA-Z]+\", i) and last_is_chinese:\n",
        "            sentence_new[compartments].append(i)\n",
        "        # If last is english and current is not chinese, continue with the current compartment\n",
        "        elif re.search(\"[a-zA-Z]+\", i) and not last_is_chinese:\n",
        "            sentence_new[compartments].append(i)\n",
        "        else:\n",
        "            sentence_new[compartments].append(i)\n",
        "    return sentence_new\n",
        "\n",
        "comparmentalized_chinese_major = []\n",
        "comparmentalized_english_major = []\n",
        "\n",
        "for i in chinese_major:\n",
        "    comparmentalized_chinese_major.append(compartmentalization_and_word_tokenization_2(i))\n",
        "\n",
        "for i in english_major:\n",
        "    comparmentalized_english_major.append(compartmentalization_and_word_tokenization_2(i))\n",
        "\n",
        "# Test print the first 10 lines\n",
        "for i in range(10):\n",
        "    print(i, comparmentalized_chinese_major[i])\n",
        "\n",
        "for i in range(10):\n",
        "    print(i, comparmentalized_english_major[i])\n"
      ],
      "metadata": {
        "id": "cAI0C6qPDnIC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee5df7e1-6bdd-4e3e-ba3c-b6b984f973f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [[], ['往', '下', '一', '点']]\n",
            "1 [[], ['那', '个'], ['wave'], ['比', '较', '慢', '一', '点', '啊']]\n",
            "2 [['okay'], ['可', '以']]\n",
            "3 [[], ['讲', '多', '一', '点', '咯', '可', '以']]\n",
            "4 [['okay']]\n",
            "5 [['okay'], ['他', '跟', '你', '讲', '了', '啊', '就', '是', '这', '样', '的', '咯', '都', '跟', '你', '讲', '这', '是', '这', '样', '了', '啦', '对', '不', '对']]\n",
            "6 [[], ['哦', '对', '咯', '好', '再', '继', '续', '再', '讲', '咯']]\n",
            "7 [[], ['你', '要', '那', '个'], ['chocolate'], ['你', '刚', '才', '吃', '的', '那', '个'], ['chocolate']]\n",
            "8 [[], ['我', '是', '从'], ['camp'], ['那', '边', '拿', '来', '的', '自', '从'], ['mark'], ['那', '时', '拿', '来', '了', '之', '后']]\n",
            "9 [[], ['三', '块', '二']]\n",
            "0 [['yeah']]\n",
            "1 [['so', 'he']]\n",
            "2 [['hah']]\n",
            "3 [['ha']]\n",
            "4 [['ah', 'hah', 'hah', 'he', 'really', 'did']]\n",
            "5 [['okay']]\n",
            "6 [['the', 'knight', 'rider', 'brought', 'down']]\n",
            "7 [['but', 'then', 'you', \"didn't\", 'get', 'the', 'rebate']]\n",
            "8 [['yeah', 'quite', 'lah', 'sort', 'of', 'lah', 'quite', 'lah']]\n",
            "9 [['clarke', 'quay', 'er'], ['怎', '么', '会'], ['bus'], ['会'], ['broke', 'down']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construct a processed sentence (i.e., a word-tokenized and POS-tagged sentence) from list_of_monolingual_sequences with list comprehension and spaCy/nltk."
      ],
      "metadata": {
        "id": "8AwPu7H1EGfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MAKE SURE TO FIRST RUN THE FIRST CODE BLOCK AT THE TOP OF THE PAGE\n",
        "# Importation of modules\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "#Loading of the nlp Machine-Learning Model\n",
        "#Load of the nlp machine-learning model, and allocate it in the variable nlp\n",
        "\n",
        "nlp = spacy.load('zh_core_web_md')\n",
        "\n",
        "\n",
        "\n",
        "def POS_Tagger(comp_w_t_sent):\n",
        "  \"\"\"\n",
        "This function annotates a language-compartmentalized, word-tokenized sentence with abbreviated tags symbolic of lexical categories. In effect, it transforms a sentence list of monolingual fragment lists of tokens into a sentence list of monolingual fragment lists of token-tag tuples.\n",
        "  \"\"\"\n",
        "  annotated_comp_w_t_sent = []\n",
        "  for fragment in comp_w_t_sent:\n",
        "    #reference string: fragment[0]\n",
        "    # Ignore empty fragments\n",
        "    if fragment == []:\n",
        "        continue\n",
        "    m = re.search(r'[a-zA-Z]',fragment[0])\n",
        "    #We do not need to iterate through the token strings of each fragment to annotate them.\n",
        "    if m == None:#An initial Mandarin word signals an entirely Mandarin fragment.\n",
        "      #Invoke the nlp model to word-tokenize, and guarantee that the output of the nlp tokenizer is a list.\n",
        "      reassembled_fragment = \" \".join(fragment)\n",
        "      tokens = nlp(reassembled_fragment)\n",
        "      tokens = list(tokens)\n",
        "      tagged_tokens = [(str(s),s.pos_) for s in tokens]\n",
        "        #We could zip a list of tags and a list of tokens into a dictionary and   extract a list of token-tag tuples from the dictionary, though this would ruin the linear order of words.\n",
        "      #Append the list of tag-token tuples to the output list.\n",
        "      annotated_comp_w_t_sent.append(tagged_tokens)\n",
        "    if type(m) == re.Match:\n",
        "      #An initial English word signals an entirely English fragment.\n",
        "      #Call the pos_tag function of nltk. We can forgo the word_tokenize function since the fragment is already word-tokenized.\n",
        "      tagged_tokens = nltk.pos_tag(fragment)\n",
        "      #Append the list of tag-token tuples to the output list.\n",
        "      annotated_comp_w_t_sent.append(tagged_tokens)\n",
        "  return annotated_comp_w_t_sent\n",
        "\n",
        "pos_tagged_ch_major = []\n",
        "pos_tagged_en_major = []\n",
        "\n",
        "for i in comparmentalized_chinese_major:\n",
        "    pos_tagged_ch_major.append(POS_Tagger(i))\n",
        "\n",
        "for i in comparmentalized_english_major:\n",
        "    pos_tagged_en_major.append(POS_Tagger(i))\n",
        "\n",
        "# Test print the first 10 lines\n",
        "for i in range(10):\n",
        "    print(i, pos_tagged_ch_major[i])\n",
        "\n",
        "for i in range(10):\n",
        "    print(i, pos_tagged_en_major[i])"
      ],
      "metadata": {
        "id": "FEWUIazEDkhv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a716e0c-f555-4a2b-df48-699934037721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [[('往', 'ADP'), ('下', 'DET'), ('一', 'NUM'), ('点', 'NUM')]]\n",
            "1 [[('那', 'DET'), ('个', 'NUM')], [('wave', 'NN')], [('比', 'ADP'), ('较', 'ADV'), ('慢', 'VERB'), ('一', 'NUM'), ('点', 'NUM'), ('啊', 'PART')]]\n",
            "2 [[('okay', 'NN')], [('可', 'VERB'), ('以', 'ADP')]]\n",
            "3 [[('讲', 'VERB'), ('多', 'ADV'), ('一', 'NUM'), ('点', 'NUM'), ('咯', 'PART'), ('可', 'VERB'), ('以', 'ADP')]]\n",
            "4 [[('okay', 'NN')]]\n",
            "5 [[('okay', 'NN')], [('他', 'PRON'), ('跟', 'ADP'), ('你', 'PRON'), ('讲', 'VERB'), ('了', 'PART'), ('啊', 'PART'), ('就', 'ADV'), ('是', 'VERB'), ('这', 'DET'), ('样', 'NUM'), ('的', 'PART'), ('咯', 'PART'), ('都', 'ADV'), ('跟', 'ADP'), ('你', 'PRON'), ('讲', 'VERB'), ('这', 'PRON'), ('是', 'VERB'), ('这', 'DET'), ('样', 'NUM'), ('了', 'PART'), ('啦', 'PART'), ('对', 'VERB'), ('不', 'ADV'), ('对', 'VERB')]]\n",
            "6 [[('哦', 'INTJ'), ('对', 'VERB'), ('咯', 'INTJ'), ('好', 'VERB'), ('再', 'ADV'), ('继', 'ADP'), ('续', 'NOUN'), ('再', 'ADV'), ('讲', 'VERB'), ('咯', 'PART')]]\n",
            "7 [[('你', 'PRON'), ('要', 'VERB'), ('那', 'DET'), ('个', 'NUM')], [('chocolate', 'NN')], [('你', 'PRON'), ('刚', 'ADV'), ('才', 'ADV'), ('吃', 'VERB'), ('的', 'PART'), ('那', 'DET'), ('个', 'NUM')], [('chocolate', 'NN')]]\n",
            "8 [[('我', 'PRON'), ('是', 'VERB'), ('从', 'ADP')], [('camp', 'NN')], [('那', 'DET'), ('边', 'ADV'), ('拿', 'VERB'), ('来', 'VERB'), ('的', 'PART'), ('自', 'ADV'), ('从', 'VERB')], [('mark', 'NN')], [('那', 'ADV'), ('时', 'PART'), ('拿', 'VERB'), ('来', 'VERB'), ('了', 'PART'), ('之', 'PART'), ('后', 'NOUN')]]\n",
            "9 [[('三', 'NUM'), ('块', 'NUM'), ('二', 'NUM')]]\n",
            "0 [[('yeah', 'NN')]]\n",
            "1 [[('so', 'RB'), ('he', 'PRP')]]\n",
            "2 [[('hah', 'NN')]]\n",
            "3 [[('ha', 'NN')]]\n",
            "4 [[('ah', 'NN'), ('hah', 'NN'), ('hah', 'NN'), ('he', 'PRP'), ('really', 'RB'), ('did', 'VBD')]]\n",
            "5 [[('okay', 'NN')]]\n",
            "6 [[('the', 'DT'), ('knight', 'NN'), ('rider', 'NN'), ('brought', 'VBD'), ('down', 'RP')]]\n",
            "7 [[('but', 'CC'), ('then', 'RB'), ('you', 'PRP'), (\"didn't\", 'VBP'), ('get', 'VB'), ('the', 'DT'), ('rebate', 'NN')]]\n",
            "8 [[('yeah', 'UH'), ('quite', 'RB'), ('lah', 'JJ'), ('sort', 'NN'), ('of', 'IN'), ('lah', 'NN'), ('quite', 'RB'), ('lah', 'JJ')]]\n",
            "9 [[('clarke', 'NN'), ('quay', 'NN'), ('er', 'NN')], [('怎', 'ADV'), ('么', 'PART'), ('会', 'VERB')], [('bus', 'NN')], [('会', 'VERB')], [('broke', 'VBD'), ('down', 'RP')]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computation of the Number of switched English Nouns, Adjectives, and Verbs"
      ],
      "metadata": {
        "id": "H--JzkU4DkqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def eng_number(annotated_comp_w_t_sent):\n",
        "  \"\"\"\n",
        "Absorbing a part of speech-annotated, language-compartmentalized, and word-tokenized list of lists of tuples as an input, this function totals up the number of switched English fragment-initial nouns, switched English fragment-initial adjectives, and switched English fragment-initial verbs, integrating them into a three-element list output.\n",
        "  \"\"\"\n",
        "  eng_number_values = [0,0,0] #Define all value-storing lists prior to the for loop,   so as to not override them with each iteration of the loop.\n",
        "  # Eliminate the sentence-initial monolingual fragment, which does not follow a code-switching point. \n",
        "  # Do not pop data from the list, that will change the original list and cause error\n",
        "  # annotated_comp_w_t_sent.reverse()\n",
        "  # first_monolingual_segment = annotated_comp_w_t_sent.pop()\n",
        "  # annotated_comp_w_t_sent.reverse()\n",
        "  # Use this code instead\n",
        "  annotated_comp_w_t_sent_new = annotated_comp_w_t_sent[1:]\n",
        "  for fragment in annotated_comp_w_t_sent_new:\n",
        "    #reference string: fragment[0][0]\n",
        "    m = re.search(r'[a-zA-Z]',fragment[0][0]) #0th component of 0th tuple in list\n",
        "    if type(m) == re.Match: #An English first word signals an entirely English fragment.\n",
        "      #Note: lists are mutable, so list indices can be reallocated new values.\n",
        "      if  fragment[0][1] == 'NN': #1st component of 0th tuple in list\n",
        "        eng_number_values[0] = eng_number_values[0]+1\n",
        "      if  fragment[0][1] == 'JJ': #1st component of 0th tuple in list\n",
        "        eng_number_values[1] = eng_number_values[0]+1\n",
        "      if  fragment[0][1] == 'VB' or fragment[0][1] == 'VBP' or fragment[0][1] == 'VBZ' or fragment[0][1] == 'VBN' or fragment[0][1] == 'VBD': #1st component of 0th tuple in list\n",
        "        eng_number_values[2] = eng_number_values[0]+1\n",
        "    else: #A Mandarin first word signals an entirely Mandarin fragment.\n",
        "      pass\n",
        "  return eng_number_values\n",
        "\n",
        "eng_number_lst = [0,0,0]\n",
        "\n",
        "#Total up the number of switched entirely English fragment-initial nouns, adjectives, and verbs in the Mandarin-dominated dataset.\n",
        "\n",
        "# Loop through pos_tagged_ch_major\n",
        "for i in pos_tagged_ch_major:\n",
        "    temp = eng_number(i)\n",
        "    for i in range(3):\n",
        "        eng_number_lst[i]+= temp[i]\n",
        "        \n",
        "print(eng_number_lst)\n",
        "   \n",
        "    "
      ],
      "metadata": {
        "id": "ZIO7mRIaDk3C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60062278-d506-4c3d-c6b4-263199379366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4856, 978, 423]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computation of the Number of switched Mandarin Nouns, Adjectives, and Verbs"
      ],
      "metadata": {
        "id": "dp8vsuT_DlAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chin_number(annotated_comp_w_t_sent):\n",
        "  \"\"\"\n",
        "Absorbing a part of speech-annotated, language-compartmentalized, and word-tokenized list of lists of tuples as an input, this function totals up the number of switched English fragment-initial nouns, switched English fragment-initial adjectives, and switched English fragment-initial verbs, integrating them into a three-element list output.\n",
        "  \"\"\"\n",
        "  #Eliminate the sentence-initial monolingual fragment, which does not follow a code-switching point.\n",
        "  # annotated_comp_w_t_sent.reverse()\n",
        "  # first_monolingual_segment = annotated_comp_w_t_sent.pop()\n",
        "  # annotated_comp_w_t_sent.reverse()\n",
        "  # Use this code instead\n",
        "  annotated_comp_w_t_sent_new = annotated_comp_w_t_sent[1:]\n",
        "  chin_number_values = [0,0,0] #Define all value-storing lists prior to the for loop, so as to not override them with each iteration of the loop.\n",
        "  for fragment in annotated_comp_w_t_sent_new:\n",
        "    #reference string: fragment[0]\n",
        "    m = re.search(r'[a-zA-Z]',fragment[0][0])\n",
        "    if m == None: #A Mandarin first word signals an entirely Mandarin fragment.\n",
        "      #Note: lists are mutable, so list indices can be reallocated new values.\n",
        "      if  fragment[0][1] == 'NOUN':\n",
        "        chin_number_values[0]+=1\n",
        "      if  fragment[0][1] == 'ADJ':\n",
        "        chin_number_values[1]+=1\n",
        "      if  fragment[0][1] == 'VERB':\n",
        "        chin_number_values[2]+=1\n",
        "    else: #An English first word signals an entirely English fragment.\n",
        "      pass\n",
        "  return chin_number_values\n",
        "\n",
        "\n",
        "#print(chin_number([[('你', 'PRON'), ('要', 'VERB'), ('那', 'DET'), ('个', 'NUM')], [('chocolate', 'NN')], [('你', 'PRON'), ('刚', 'ADV'), ('才', 'ADV'), ('吃', 'VERB'), ('的', 'PART'), ('那', 'DET'), ('个', 'NUM')], [('chocolate', 'NN')], [('你', 'PRON')]]))\n",
        "#print(chin_number([[('我', 'PRON'), ('是', 'VERB'), ('从', 'ADP')], [('camp', 'NN')], [('那', 'DET'), ('边', 'ADV'), ('拿', 'VERB'), ('来', 'VERB'), ('的', 'PART'), ('自', 'ADV'), ('从', 'VERB')], [('mark', 'NN')], [('那', 'ADV'), ('时', 'PART'), ('拿', 'VERB'), ('来', 'VERB'), ('了', 'PART'), ('之', 'PART'), ('后', 'NOUN')]]))\n",
        "\n",
        "\n",
        "\n",
        "#Three-element tuples signify the counts of switched Mandarin segment-initial nouns, adjectives, and verbs in a particular sentence.\n",
        "\n",
        "chin_number_1st = [0,0,0]\n",
        "\n",
        "\n",
        "#Total up the number of switched entirely Mandarin fragment-initial nouns, adjectives, and verbs in the English-dominated dataset.\n",
        "\n",
        "for i in pos_tagged_en_major:\n",
        "  temp = chin_number(i)\n",
        "  for j in range(3):\n",
        "    chin_number_1st[j]+=temp[j]\n",
        "\n",
        "\n",
        "print(chin_number_1st)\n"
      ],
      "metadata": {
        "id": "a0tdTRlmDlUl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43696be1-bffe-4d6d-f9a1-ce549bc1e9f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[63, 19, 304]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a bar chart or other diagram in excel, colab, etc that can visually illustrate the number of switched words that are adjectives, nouns, and verbs and the number of switched words that are adjectives, nouns, and verbs.\n",
        "\n"
      ],
      "metadata": {
        "id": "wzLfoqGfDlcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "0XVXp-LB2r8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bar_chart(eng_number, chin_number):\n",
        "    left = np.array([1, 2])\n",
        "    labels = [\"Chinese Switched to English\", \"English Switched to Chinese\"]\n",
        "    nouns_bar = np.array([eng_number[0], chin_number[0]])\n",
        "    adjs_bar = np.array([eng_number[1], chin_number[1]])\n",
        "    verbs_bar = np.array([eng_number[2], chin_number[2]])\n",
        "    p1 = plt.bar(left, nouns_bar, color=\"green\", tick_label = labels)\n",
        "    p2 = plt.bar(left, adjs_bar, bottom=nouns_bar, color=\"orange\")\n",
        "    p3 = plt.bar(left, verbs_bar, bottom=adjs_bar+nouns_bar, color=\"red\")\n",
        "    plt.legend((p1[0], p2[0], p3[0]), (\"NOUN / NN\", \"ADJ / JJ\", \"VERB / VB\"))\n",
        "    plt.ylabel(\"Number of POS\")\n",
        "    plt.title(\"The numbers of each POS in English and Chinese\")\n",
        "    plt.show()\n",
        "\n",
        "bar_chart(eng_number_lst,chin_number_1st)"
      ],
      "metadata": {
        "id": "iA4niW3tEnbl",
        "outputId": "c68d055c-2889-4277-98d9-78f2ecf3bf20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}